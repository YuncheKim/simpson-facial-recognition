# -*- coding: utf-8 -*-
"""simpsons facial recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oTs3v-qBF8ttVpqZcymbL-4bzZNi-d6K
"""

!pip install canaro caer

import cv2 as cv
import caer
import canaro
import numpy as np
import gc
import os

IMG_SIZE=(180,180)
channels= 1
char_path=r'drive/MyDrive/the-simpsons-characters-dataset/simpsons_dataset'

char_dict = {}

for char in os.listdir(char_path):
    char_dict[char] = len(os.listdir(os.path.join(char_path, char)))


char_dict = caer.sort_dict(char_dict, descending = True)

char_dict

char_dict = {}

for char in os.listdir(char_path):
    char_dict[char] = len(os.listdir(os.path.join(char_path, char)))

# Sortin in descending order
char_dict = caer.sort_dict(char_dict, descending = True)

char_dict

characters = []
count = 0

for i in char_dict:
    characters.append(i[0])
    count += 1

    if count >= 10:
        break

characters

train = caer.preprocess_from_dir(char_path, characters, channels=channels,
                                 IMG_SIZE = IMG_SIZE, isShuffle = True)

len(train)



import matplotlib.pyplot as plt
plt.figure(figsize = (2,2))
plt.imshow(train[0][0], cmap = 'gray')
plt.show()

featureSet, labels = caer.sep_train(train,IMG_SIZE = IMG_SIZE)

from tensorflow.keras.utils import to_categorical

# Normalize the featureset ---> (0.1)

featureSet = caer.normalize(featureSet)
labels = to_categorical(labels, len(characters))

x_train, x_val, y_train, y_val = caer.train_val_split(featureSet, labels, val_ratio = 0.2)

del featureSet
del train
del labels

gc.collect()

BATCH_SIZE = 10
EPOCHS = 50

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Create an instance of the ImageDataGenerator with specified augmentations
datagen = ImageDataGenerator(
    rotation_range=30,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.2
)

# Flow the training data through the generator
train_gen = datagen.flow(x_train, y_train, batch_size=BATCH_SIZE)

import tensorflow as tf
from keras.layers import Conv2D, Flatten, MaxPooling2D, Dense, Input, Dropout

def create_ST_layer(input_shape=(180, 180, 1)):
    input_img = Input(shape=input_shape)
    model = Conv2D(512, kernel_size=(7, 7), input_shape=input_shape, strides=(1, 1), activation="relu")(input_img)
    model = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(model)
    model = Conv2D(512, kernel_size=(7, 7), strides=(1, 1), activation="relu")(model)
    model = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(model)
    model = Conv2D(512, kernel_size=(7, 7), strides=(1, 1), activation="relu")(model)
    model = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(model)
    model = Conv2D(512, kernel_size=(7, 7), strides=(1, 1), activation="relu")(model)
    model = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(model)

    model = Flatten()(model)
    model = Dense(512, activation="sigmoid")(model)
    model = Dropout(0.5)(model)
    model = Dense(512, activation="relu")(model)
    model = Dropout(0.5)(model)
    model = Dense(6, activation="sigmoid")(model)

    model = tf.keras.Model(inputs=input_img, outputs=model)
    return model

# Print the model summary
model = create_ST_layer()
model.summary()

from tensorflow.keras.callbacks import LearningRateScheduler
callbacks_list=[LearningRateScheduler(canaro.lr_schedule)]

from tensorflow.keras.optimizers import SGD

# Assuming you have the necessary data and model defined
# ...

# Create the SGD optimizer
optimizer = tf.keras.optimizers.SGD(learning_rate=0.0000001, decay=1e-7, momentum=0.7, nesterov=True)


# Compile the model with the optimizer and loss function
model.compile(optimizer, loss='binary_crossentropy')

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, Flatten, Dense

model = Sequential()

# Adjust the input shape in the first layer based on your data
height=180
width=180
model.add(Conv2D(32, kernel_size=(3, 3), input_shape=(height, width, channels), activation='relu'))
model.add(Flatten())
model.add(Dense(10, activation='sigmoid'))

# Compile the model and proceed with training
model.compile(optimizer, loss='binary_crossentropy', metrics=['accuracy'])

training=model.fit(train_gen, steps_per_epoch=len(x_train)//BATCH_SIZE,
                   epochs=EPOCHS, validation_data=(x_val, y_val),
                   validation_steps=len(y_val)//BATCH_SIZE, callbacks=callbacks_list)



test_path = r'drive/MyDrive/the-simpsons-characters-dataset/simpsons_dataset/krusty_the_clown/pic_0041.jpg'

img = cv.imread(test_path)

plt.imshow(img)
plt.show()

def prepare(image):
    image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)
    image = cv.resize(image, IMG_SIZE)
    image = caer.reshape(image, IMG_SIZE, 1)
    return image

predictions = model.predict(prepare(img))

print(characters[np.argmax(predictions[0])])

